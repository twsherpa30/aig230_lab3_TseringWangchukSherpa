{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc81b3e",
   "metadata": {},
   "source": [
    "# AIG230 NLP (Week 3 Lab) — Notebook 1: Text Representation\n",
    "\n",
    "This notebook focuses on **turning raw text into numeric features** you can use in real-world ML systems.\n",
    "\n",
    "You will build:\n",
    "- a clean **train/test split**\n",
    "- **Bag-of-Words** (binary and count)\n",
    "- **Document-Term Matrix** (DTM)\n",
    "- **TF-IDF** (with n-grams)\n",
    "- **Hashing trick** (production-friendly)\n",
    "- basic **retrieval** (cosine similarity) and a **baseline classifier**\n",
    "- model **persistence** (save/load)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31f235f",
   "metadata": {},
   "source": [
    "## 0) Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2bc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5ad879",
   "metadata": {},
   "source": [
    "## 1) A small, realistic dataset (you can replace with your own CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88fd94a",
   "metadata": {},
   "source": [
    "In industry, text often comes with:\n",
    "- an **ID**\n",
    "- free-text **description**\n",
    "- a **label** (category, priority, intent, topic) or a target (churn, fraud, etc.)\n",
    "\n",
    "Here we create a toy dataset that looks like support tickets / ops incidents.  \n",
    "Swap this section with a `pd.read_csv(...)` in your own workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2912b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [\n",
    "    (\"T-001\", \"VPN keeps disconnecting every 10 minutes on Windows 11 after latest update\", \"network\"),\n",
    "    (\"T-002\", \"Password reset link is expired and user cannot login to the portal\", \"auth\"),\n",
    "    (\"T-003\", \"Email delivery delayed, outbound messages queued for hours\", \"messaging\"),\n",
    "    (\"T-004\", \"Cannot install printer driver, installer fails with error code 1603\", \"device\"),\n",
    "    (\"T-005\", \"MFA prompt never arrives on mobile app, user stuck at login\", \"auth\"),\n",
    "    (\"T-006\", \"WiFi signal drops in meeting rooms, access point reboot helps temporarily\", \"network\"),\n",
    "    (\"T-007\", \"Outlook search not returning results, index seems corrupted\", \"messaging\"),\n",
    "    (\"T-008\", \"Laptop battery drains fast after BIOS update, power settings unchanged\", \"device\"),\n",
    "    (\"T-009\", \"Portal shows 500 error when submitting form, happened after deployment\", \"app\"),\n",
    "    (\"T-010\", \"API requests timing out, latency spike observed in last hour\", \"app\"),\n",
    "    (\"T-011\", \"User cannot access shared drive, permission denied though in correct group\", \"auth\"),\n",
    "    (\"T-012\", \"Teams calls have choppy audio, jitter high on corporate network\", \"network\"),\n",
    "    (\"T-013\", \"Push notifications not working on Android for the app\", \"app\"),\n",
    "    (\"T-014\", \"Mailbox is full and cannot receive emails, auto-archive not running\", \"messaging\"),\n",
    "    (\"T-015\", \"Bluetooth mouse not pairing after restart, device shows as unknown\", \"device\"),\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"ticket_id\", \"text\", \"label\"])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e16c08",
   "metadata": {},
   "source": [
    "### Train/test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b923b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.33, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(X_train))\n",
    "print(\"Test size:\", len(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe83e41",
   "metadata": {},
   "source": [
    "## 2) Tokenization basics and normalization (lightweight, practical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166ac38a",
   "metadata": {},
   "source": [
    "In production pipelines you typically do **minimal, safe normalization**:\n",
    "- lowercase\n",
    "- normalize whitespace\n",
    "- optionally strip obvious punctuation\n",
    "- keep numbers when they carry meaning (error codes, versions, dates)\n",
    "\n",
    "Heavy normalization (stemming, aggressive regexes) can hurt when your text includes:\n",
    "error codes, product names, IDs, or domain terminology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517e7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_normalize(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"text_norm\"] = df[\"text\"].map(simple_normalize)\n",
    "df[[\"ticket_id\",\"text_norm\",\"label\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a8087e",
   "metadata": {},
   "source": [
    "## 3) Vocabulary + Document-Term Matrix (DTM) with CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cad1ae",
   "metadata": {},
   "source": [
    "**CountVectorizer** builds:\n",
    "- a vocabulary (token → column index)\n",
    "- a sparse matrix where rows are documents and columns are tokens\n",
    "\n",
    "This is the classic **Document-Term Matrix** representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e35424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "count_vec = CountVectorizer(\n",
    "    lowercase=True,\n",
    "    token_pattern=r\"(?u)\\b\\w+\\b\",  # keeps tokens like \"500\", \"1603\", \"mfa\"\n",
    "    min_df=1\n",
    ")\n",
    "\n",
    "X_train_counts = count_vec.fit_transform(X_train)\n",
    "X_test_counts  = count_vec.transform(X_test)\n",
    "\n",
    "print(\"DTM shape (train):\", X_train_counts.shape)\n",
    "print(\"Vocabulary size:\", len(count_vec.vocabulary_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03137003",
   "metadata": {},
   "source": [
    "### Inspect the vocabulary and a single row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48545c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Show a small slice of the vocabulary (token -> index)\n",
    "vocab_items = sorted(count_vec.vocabulary_.items(), key=lambda x: x[1])[:25]\n",
    "vocab_items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e5357c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Look at a specific document row: non-zero entries (token counts)\n",
    "row_id = 0\n",
    "row = X_train_counts[row_id]\n",
    "inv_vocab = {idx: tok for tok, idx in count_vec.vocabulary_.items()}\n",
    "\n",
    "nz_cols = row.nonzero()[1]\n",
    "tokens_counts = sorted([(inv_vocab[c], int(row[0, c])) for c in nz_cols], key=lambda x: -x[1])\n",
    "tokens_counts[:20]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25e4094",
   "metadata": {},
   "source": [
    "## 4) Binary vs Count-based Bag-of-Words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e35af",
   "metadata": {},
   "source": [
    "Binary BoW: token present or not (good for short texts and some classification tasks)  \n",
    "Count BoW: raw frequency (baseline for many pipelines)\n",
    "\n",
    "Both discard word order.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8060cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888da90",
   "metadata": {},
   "source": [
    "## 5) TF-IDF (a refinement, not a replacement)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa10e225",
   "metadata": {},
   "source": [
    "TF-IDF downweights very common tokens and upweights tokens that are more distinctive.\n",
    "\n",
    "In industry, TF-IDF with **n-grams** is a strong baseline for:\n",
    "- ticket routing\n",
    "- intent detection\n",
    "- spam detection\n",
    "- incident clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dfd321",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d8d93",
   "metadata": {},
   "source": [
    "## 6) Quick retrieval: 'find similar tickets' with cosine similarity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51871c0e",
   "metadata": {},
   "source": [
    "A very common industry use case is **nearest neighbor retrieval** for:\n",
    "- deduplication\n",
    "- suggesting knowledge base articles\n",
    "- finding similar past incidents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae445ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f488cc10",
   "metadata": {},
   "source": [
    "## 7) Classification baseline (Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384a481a",
   "metadata": {},
   "source": [
    "For text classification, a strong baseline is:\n",
    "\n",
    "**TF-IDF → Linear model (LogReg / Linear SVM)**\n",
    "\n",
    "This is fast, reliable, easy to explain, and often hard to beat without deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b472a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = LogisticRegression(max_iter=2000)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer(\n",
    "        ngram_range=(1,2),\n",
    "        token_pattern=r\"(?u)\\b\\w+\\b\",\n",
    "        sublinear_tf=True\n",
    "    )),\n",
    "    (\"model\", clf)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "pred = pipeline.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc3b2be",
   "metadata": {},
   "source": [
    "## 8) Production pattern: HashingVectorizer (no stored vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2aadab9",
   "metadata": {},
   "source": [
    "In production, you may need:\n",
    "- constant memory usage\n",
    "- privacy (no vocabulary inspection)\n",
    "- streaming support\n",
    "- easier deployment across services\n",
    "\n",
    "**HashingVectorizer** avoids building a vocabulary. Tradeoff: collisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c023926",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae2433",
   "metadata": {},
   "source": [
    "## 9) Save and load the model (typical deployment step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9a1fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fffc4fb",
   "metadata": {},
   "source": [
    "## Exercises (do these during lab)\n",
    "1) Add 10 more tickets to `data` with realistic wording and labels. Re-train and compare results.  \n",
    "2) Try `ngram_range=(1,3)` and observe what changes.  \n",
    "3) For retrieval, test at least 3 queries and explain why the top result makes sense.  \n",
    "4) Replace the dataset with a CSV you create (columns: `text`, `label`) and rerun the notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aig230-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
